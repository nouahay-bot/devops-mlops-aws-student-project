# ğŸŒ¸ Iris Classification API \- DevOps-MLOps Pipeline

**Pipeline DevOps-MLOps complet** pour l'entraÃ®nement, la conteneurisation et le dÃ©ploiement d'un modÃ¨le de machine learning en production sur AWS.

---

## ğŸ“‹ Table des matiÃ¨res

- [Vue d'ensemble](#vue-densemble)  
- [Architecture](#architecture)  
- [PrÃ©requis](#prÃ©requis)  
- [Installation](#installation)  
- [Utilisation](#utilisation)  
- [API Endpoints](#api-endpoints)  
- [Tests](#tests)  
- [DÃ©ploiement](#dÃ©ploiement)  
- [Structure du projet](#structure-du-projet)  
- [Contribution](#contribution)

---

## ğŸ¯ Vue d'ensemble

Ce projet dÃ©montre un pipeline **MLOps-DevOps moderne** complet :

âœ… **Phase 1 \- MLOps** : EntraÃ®nement d'un modÃ¨le Decision Tree sur le dataset Iris  
âœ… **Phase 2 \- DevOps** : Conteneurisation avec Docker et automatisation CI/CD  
âœ… **Phase 3 \- Infrastructure** : DÃ©ploiement sur AWS EC2

### Cas d'usage

PrÃ©diction de l'espÃ¨ce d'une fleur Iris basÃ©e sur 4 caractÃ©ristiques (longueur/largeur des sÃ©pales et pÃ©tales).

### ModÃ¨le utilisÃ©

- **Type** : Decision Tree Classifier  
- **Accuracy** : 96.67%  
- **Precision** : 96.97%  
- **Recall** : 96.67%  
- **Dataset** : Iris (150 observations, 3 classes, 4 features)

---

## ğŸ—ï¸ Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚                 PIPELINE DEVOPS-MLOPS COMPLET               â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PHASE 1: MLOps (EntraÃ®nement)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

AWS SageMaker Notebook

    â†“

Dataset Iris

    â†“

EntraÃ®nement Decision Tree

    â†“

Sauvegarde model.pkl \+ scaler.pkl

PHASE 2: DevOps (DÃ©veloppement & CI/CD)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Code source (app.py, requirements.txt)

    â†“

GitHub Repository

    â†“

GitHub Actions CI/CD

    â†“

Build Docker Image (425 MB)

    â†“

Push DockerHub/ECR

PHASE 3: DÃ©ploiement (Infrastructure)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

AWS EC2 Instance (t2.micro)

    â†“

Docker Run

    â†“

Flask API (Port 5000\)

    â†“

PrÃ©dictions en Production âœ“

---

## ğŸ“¦ PrÃ©requis

### Outils requis

- **Docker** \>= 20.10  
- **Python** \>= 3.11  
- **Git**  
- **AWS Account** (pour dÃ©ploiement EC2)

### Installation des outils

**Windows (PowerShell):**

\# Installer Docker Desktop

choco install docker-desktop \-y

\# VÃ©rifier l'installation

docker \--version

docker ps

**macOS:**

brew install docker

docker \--version

**Linux (Ubuntu):**

sudo apt-get update

sudo apt-get install docker.io \-y

sudo usermod \-aG docker $USER

---

## ğŸš€ Installation

### 1\. Cloner le repository

git clone https://github.com/username/devops-mlops-aws-student-project.git

cd devops-mlops-aws-student-project

### 2\. Installer les dÃ©pendances Python (local)

\# CrÃ©er un environnement virtuel

python \-m venv venv

\# Activer l'environnement

\# Windows

venv\\Scripts\\activate

\# macOS/Linux

source venv/bin/activate

\# Installer les dÃ©pendances

pip install \-r requirements.txt

### 3\. VÃ©rifier les fichiers du modÃ¨le

ls model/

\# Devrait contenir :

\# \- model.pkl (1.2 MB)

\# \- scaler.pkl (0.8 KB)

---

## ğŸ’» Utilisation

### Option 1 : ExÃ©cution locale (Python)

\# Activer l'environnement virtuel

source venv/bin/activate  \# macOS/Linux

\# ou

venv\\Scripts\\activate  \# Windows

\# Lancer l'API

python \-m api

\# L'API dÃ©marre sur http://localhost:5000

### Option 2 : ExÃ©cution avec Docker (RecommandÃ©)

\# Build l'image Docker

docker build \-t ml-api:1.0 .

\# Lancer le conteneur

docker run \-d \\

  \-p 5000:5000 \\

  \--name ml-api-container \\

  \--restart unless-stopped \\

  ml-api:1.0

\# VÃ©rifier que le conteneur tourne

docker ps

\# Voir les logs

docker logs \-f ml-api-container

### Option 3 : Docker Compose (si disponible)

\# Lancer les services

docker-compose up \-d

\# ArrÃªter les services

docker-compose down

---

## ğŸ”Œ API Endpoints

### 1\. Health Check

GET /health

**Response (200 OK):**

{

  "status": "healthy",

  "model\_loaded": true,

  "timestamp": "2024-12-24T10:30:45.123456"

}

**UtilitÃ©** : VÃ©rifier que l'API rÃ©pond (health checks, load balancers)

---

### 2\. PrÃ©diction (Endpoint principal)

POST /predict

Content-Type: application/json

{

  "features": \[5.1, 3.5, 1.4, 0.2\]

}

**Response (200 OK):**

{

  "prediction": "Setosa",

  "class\_id": 0,

  "probabilities": {

    "Setosa": 0.99,

    "Versicolor": 0.01,

    "Virginica": 0.0

  },

  "confidence": 0.99,

  "timestamp": "2024-12-24T10:31:20.654321"

}

**Parameters:**

- `features` (array\[4\]) : 4 nombres float  
  - `features[0]` : Sepal length (cm)  
  - `features[1]` : Sepal width (cm)  
  - `features[2]` : Petal length (cm)  
  - `features[3]` : Petal width (cm)

**Returns:**

- `prediction` : Classe prÃ©dite ("Setosa", "Versicolor", "Virginica")  
- `class_id` : ID de la classe (0, 1, ou 2\)  
- `probabilities` : ProbabilitÃ©s pour chaque classe  
- `confidence` : Confiance de la prÃ©diction (max des probabilitÃ©s)

---

### 3\. Informations API

GET /info

**Response (200 OK):**

{

  "app\_name": "Iris Classification API",

  "version": "1.0.0",

  "model\_type": "Decision Tree Classifier",

  "dataset": "Iris Dataset",

  "classes": \["Setosa", "Versicolor", "Virginica"\],

  "num\_features": 4,

  "feature\_names": \[

    "sepal length (cm)",

    "sepal width (cm)",

    "petal length (cm)",

    "petal width (cm)"

  \],

  "endpoints": {

    "GET /health": "VÃ©rifier l'Ã©tat du service",

    "POST /predict": "Faire une prÃ©diction",

    "GET /info": "Informations sur l'API"

  }

}

---

## ğŸ§ª Tests

### Test 1 : Health Check

curl http://localhost:5000/health

**Expected:** Status 200, model\_loaded: true

---

### Test 2 : PrÃ©diction correcte (Setosa)

curl \-X POST http://localhost:5000/predict \\

  \-H "Content-Type: application/json" \\

  \-d '{"features": \[5.1, 3.5, 1.4, 0.2\]}'

**Expected:**

- Status 200  
- prediction: "Setosa"  
- confidence: 0.99

---

### Test 3 : PrÃ©diction (Versicolor)

curl \-X POST http://localhost:5000/predict \\

  \-H "Content-Type: application/json" \\

  \-d '{"features": \[6.5, 2.8, 4.6, 1.5\]}'

**Expected:** prediction: "Versicolor"

---

### Test 4 : PrÃ©diction (Virginica)

curl \-X POST http://localhost:5000/predict \\

  \-H "Content-Type: application/json" \\

  \-d '{"features": \[7.6, 3.0, 6.6, 2.2\]}'

**Expected:** prediction: "Virginica"

---

### Test 5 : Erreur de validation

curl \-X POST http://localhost:5000/predict \\

  \-H "Content-Type: application/json" \\

  \-d '{"features": \[5.1, 3.5\]}'

**Expected:**

- Status 400 Bad Request  
- error: "Nombre de features invalide"

---

### Tests avec Postman

Importer la collection Postman : `tests/postman_collection.json`

\# Ou lancer les tests automatiquement

pytest tests/test\_api.py \-v

---

## ğŸŒ DÃ©ploiement

### DÃ©ploiement sur AWS EC2

#### Ã‰tape 1 : CrÃ©er une instance EC2

\# AWS Console

1\. EC2 â†’ Instances â†’ Launch Instance

2\. AMI : Ubuntu 22.04 LTS

3\. Instance Type : t2.micro (free tier)

4\. Security Group : Ouvrir ports 22 (SSH) et 5000 (HTTP)

5\. Key Pair : TÃ©lÃ©charger la clÃ© .pem

#### Ã‰tape 2 : Connexion SSH

ssh \-i "your-key.pem" ubuntu@\<EC2\_PUBLIC\_IP\>

#### Ã‰tape 3 : Installation de Docker

\# Mettre Ã  jour le systÃ¨me

sudo apt-get update && sudo apt-get upgrade \-y

\# Installer Docker

sudo apt-get install \-y docker.io

\# Ajouter l'utilisateur au groupe docker

sudo usermod \-aG docker $USER

newgrp docker

\# VÃ©rifier l'installation

docker \--version

#### Ã‰tape 4 : DÃ©ployer le conteneur

\# Pull l'image depuis DockerHub

docker pull username/ml-api:latest

\# Lancer le conteneur

docker run \-d \\

  \-p 5000:5000 \\

  \--name ml-api-prod \\

  \--restart unless-stopped \\

  username/ml-api:latest

\# VÃ©rifier

docker ps

#### Ã‰tape 5 : Tester en production

\# Depuis votre machine locale

curl http://\<EC2\_PUBLIC\_IP\>:5000/health

curl \-X POST http://\<EC2\_PUBLIC\_IP\>:5000/predict \\

  \-H "Content-Type: application/json" \\

  \-d '{"features": \[5.1, 3.5, 1.4, 0.2\]}'

---

## ğŸ“ Structure du projet

devops-mlops-aws-student-project/

â”œâ”€â”€ README.md                          \# Ce fichier

â”œâ”€â”€ .gitignore                         \# Fichiers Ã  ignorer dans Git

â”œâ”€â”€ .dockerignore                      \# Fichiers Ã  ignorer dans Docker

â”‚

â”œâ”€â”€ requirements.txt                   \# DÃ©pendances Python

â”œâ”€â”€ Dockerfile                         \# Configuration Docker

â”œâ”€â”€ docker-compose.yml                 \# (Optionnel) Orchestration containers

â”‚

â”œâ”€â”€ api/                               \# Code de l'API Flask

â”‚   â”œâ”€â”€ \_\_init\_\_.py                    \# Initialisation Flask

â”‚   â”œâ”€â”€ app.py                         \# Application Flask principale

â”‚   â”œâ”€â”€ model\_loader.py                \# Chargement des modÃ¨les ML

â”‚   â”œâ”€â”€ routes.py                      \# Endpoints de l'API

â”‚   â””â”€â”€ config.py                      \# Configuration

â”‚

â”œâ”€â”€ model/                             \# ModÃ¨les ML sÃ©rialisÃ©s

â”‚   â”œâ”€â”€ model.pkl                      \# Decision Tree entraÃ®nÃ©

â”‚   â””â”€â”€ scaler.pkl                     \# StandardScaler pour normalisation

â”‚

â”œâ”€â”€ notebooks/                         \# Jupyter Notebooks

â”‚   â””â”€â”€ train\_model.ipynb              \# Notebook d'entraÃ®nement

â”‚

â”œâ”€â”€ tests/                             \# Tests unitaires

â”‚   â”œâ”€â”€ test\_api.py                    \# Tests des endpoints

â”‚   â””â”€â”€ postman\_collection.json        \# Collection Postman

â”‚

â”œâ”€â”€ .github/                           \# GitHub Actions

â”‚   â””â”€â”€ workflows/

â”‚       â””â”€â”€ ci.yml                     \# Pipeline CI/CD

â”‚

â””â”€â”€ docker/                            \# (Optionnel) Configurations Docker supplÃ©mentaires

    â”œâ”€â”€ Dockerfile                     \# Alternative au Dockerfile racine

    â””â”€â”€ .dockerignore                  \# Alternative au .dockerignore racine

---

## ğŸ”„ Pipeline CI/CD (GitHub Actions)

Le pipeline s'active automatiquement lors d'un push sur `main` :

1\. Checkout Code

2\. Build Docker Image

3\. Test Image (health check)

4\. Login to DockerHub

5\. Push to DockerHub

6\. Status: âœ“ SUCCESS

**Pour dÃ©clencher manuellement :**

git push origin main

\# Voir les logs : GitHub â†’ Actions â†’ Workflow runs

---

## ğŸ“Š MÃ©triques de performance

| MÃ©trique | Valeur |
| :---- | :---- |
| **Accuracy** | 96.67% |
| **Latence API** | \< 50 ms |
| **Image Size** | 425 MB |
| **Build Time** | \~2 minutes |
| **Deployment Time** | \< 5 minutes |
| **Memory Usage** | \~200 MB |
| **CPU Usage** | \< 10% (idle) |

---

## ğŸ› ï¸ Commandes utiles

### Docker

\# Build

docker build \-t ml-api:1.0 .

\# Run

docker run \-d \-p 5000:5000 \--name ml-api ml-api:1.0

\# List containers

docker ps \-a

\# View logs

docker logs \-f ml-api

\# Stop container

docker stop ml-api

\# Remove container

docker rm ml-api

\# Remove image

docker rmi ml-api:1.0

\# Execute command in container

docker exec \-it ml-api /bin/bash

\# Inspect container

docker inspect ml-api

### Git

\# Clone repository

git clone \<repo-url\>

\# Create feature branch

git checkout \-b feature/my-feature

\# Commit changes

git add .

git commit \-m "feat: add new feature"

\# Push to GitHub

git push origin feature/my-feature

\# Create Pull Request

\# (via GitHub Web Interface  
